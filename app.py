# If the user has asked a question or uploaded a file,
if user_question or uploaded_file:

    # Construct a chat prompt template using various components
    prompt = ChatPromptTemplate.from_messages(
        [
            SystemMessage(
                content=system_prompt
            ),  # This is the persistent system prompt that is always included at the start of the chat.

            MessagesPlaceholder(
                variable_name="chat_history"
            ),  # This placeholder will be replaced by the actual chat history during the conversation. It helps in maintaining context.

            HumanMessagePromptTemplate.from_template(
                "{human_input}"
            ),  # This template is where the user's current input will be injected into the prompt.
        ]
    )

    # Create a conversation chain using the LangChain LLM (Language Learning Model)
    conversation = LLMChain(
        llm=groq_chat,  # The Groq LangChain chat object initialized earlier.
        prompt=prompt,  # The constructed prompt template.
        verbose=True,   # Enables verbose output, which can be useful for debugging.
        memory=memory,  # The conversational memory object that stores and manages the conversation history.
    )
    
    # If a file is uploaded, include it in the prompt
    if uploaded_file:
        file_content = uploaded_file.read().decode("utf-8")
        user_question = f"Please review the following document:\n\n{file_content}\n\n{user_question}"
    else:
        user_question = user_question
    
    # The chatbot's answer is generated by sending the full prompt to the Groq API.
    response = conversation.predict(human_input=user_question)
    message = {'human':user_question,'AI':response}
    st.session_state.chat_history.append(message)
    #st.write("Chatbot:", response)
    typing_effect(response)
